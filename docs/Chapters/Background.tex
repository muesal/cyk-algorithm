\section{Background}

The Cocke-Younger-Kasami algorithm, which we analyse in this report, solves the membership problem for context free grammars.
In this section we show what a context-free grammar is and how the algorithm operates on it.
Further, we introduce the three approaches for implementing the algorithm which were used in this evaluation.
\todo{And whatever we do in step 3}

\subsection{Context-Free Grammar}

Context-free grammars (CFG) can be used to formalize different types of languages.
They can, for example, be used in computer science, to define the structure of programming languages, or in linguistics to define the structure of any language.

A CFG contains a set of rules, also called productions.
Starting from a certain variable, this productions can be applied to get a sequence of terminal symbols, for example a sentence of the English language.
The sequences that can be generated with a CFG build a language, the grammars context-free language (CFL).

Formally, we define a CFG $G$ by the 4-tuple $G=(V,\Sigma,R,S)$, where $V$ are all non-terminal, and $\Sigma$ all terminal symbols.
$R$ is the set of productions and $S\in V$ is the start symbol.

The productions are of the form $A\rightarrow\alpha$, where $A$ is a variable in $V$ and $\alpha$ is a string of symbols from $(V\cup T)^*$.
If $R$ contains multiple rules for one non-terminal we abbreviate these rules as $A\rightarrow \alpha_1 | \alpha_2 | \dots |\alpha_k $, where $\alpha_i$, $i\in {1\dots k}$ is the right hand side of one of the rules for non-terminal $A$.

If for any strings $u,v\in (V\cup \Sigma^*_)$ there is a production which transforms $u$ to $v$, we say $u$ directly yields $v$, denoted as $u\Rightarrow v$.
If $v$ can be reached by applying multiple productions on $u$ we say $u$ yields $v$, denoted as $u\Rightarrow^* v$, i.e., there is a set of strings $u_1, u_2, \dots, u_k\in (V\cup\Sigma)^*)$ such that $u\Rightarrow u_1 \Rightarrow u_2 \Rightarrow \dots \Rightarrow u_k \Rightarrow v$.

The language of $G$, $L(G)$, contains all strings that can be yielded from $S$, i.e. $L(G)={w\in \Sigma^*: S\Rightarrow^{*}w}$.

The membership problem, which is solved by the CYK-algorithm, is the problem of determining whether a given string is in the language of a grammar.

The following subsections show one simple example of a context-free grammar and introduce different forms of grammars.

\subsubsection{Exemplary Grammar}
\label{subsec:exemplary_grammar}


One simple example is the grammar, whose language consists of all words of the form $(a^n b^n)$, for any $n\in \mathbb{N}$.
This grammar can be defined as $G = ({S, A, B}, {a, b}, R, S)$, where $R$ contains the following rules: 

\begin{align*}
S&\rightarrow ASB \\
A&\rightarrow a \\
B&\rightarrow b \\
\end{align*}

Any string of length $2n$ can be generated by applying $S\rightarrow ASB$ $n$ times, and then replacing all non-terminal $A$ and $B$ with $a$ and $b$ respectively.


\subsubsection{Chomsky Normal Form}

Every context-free grammar can be transformed into an equivalent representation in Chomsky normal form (CNF).
Two grammars are considered equivalent if they generate the same language.
A grammar is in CNF, if all its productions are of the form:

\hspace*{0.5cm}$A\rightarrow BC$\\
\hspace*{1cm}$A\rightarrow a$\\
\hspace*{1cm}$S\rightarrow \epsilon$\\

While $S$ is the start symbol, $A$, $B$ and $C$ are any non-terminal variables, but neither $B$ nor $C$ may be the start variable.
$a$ is a terminal variable.
The start symbol is the only variable, which may yield the empty string, provided the empty string is part of the language.
Further, a non-terminal must either yield two non terminals or one terminal variable.

In order to transform the grammar from~\ref{subsec:exemplary_grammar} to CNF, we add the non-terminals $C,D$ to $V$ and get the new set of productions $R'$:
\begin{align*}
    S&\rightarrow AB | AC \\
    C&\rightarrow DB \\
    D&\rightarrow AC|AB \\
    A&\rightarrow a \\
    B&\rightarrow b \\
\end{align*}

The CYK algorithm can only operate on grammars, that are in the (reduced) Chomsky normal form.
The reduced CNF is similar to CNF, with the only difference that $S$ may also appear on the right hand side of a production.
The following subsection describes how the CYK-algorithm operates.

\subsubsection{Linear Grammars}
\todo{define by using the book.}

\subsection{Cocke-Younger-Kasami Algorithm}

The Cocke-Younger-Kasami algorithm (CYK) solves the membership problem.
For a given Grammar $G$ and an input string $s[1..n]$, it returns the truth-value of whether the $s$ is in the $G$.
For the original algorithm, $G$ should be in reduced CNF or CNF.

The algorithm solves the membership problem in a bottom up manner.
It maintains a table $tab$ of size $n\times n$, where $tab[i,j]$ contains all non-terminals that can yield the substring of $s$ of length $j$ starting at position $i$.

First, $tab$ is initialized as an empty $n\times n$-matrix.
The algorithm then starts by looking over all substrings of size 1, to find the non-terminals that produce the terminals of the input string.
When this is done, $tab[i,1]$ contains ${A: A\in V and A\leftarrow s[i] \in R}$ for all $i\in {1\dots n}$.
The algorithms continues by increasing $j$, starting at $2$ till $n$, and iterating over all possible $i$, $1\leq i < n-j$.
Since the algorithm proceeds bottom-up, when looking at a substring of length $j$, the solution for all strings of length $j-1$ is already known.
Thus, to deduce whether non-terminal $A\in V$ can yield a substring $s[i...i+j]$ the algorithm iterates over all non-terminal productions of $A$.
For each rule $A\rightarrow BC$ of $A$ in $R$ it uses the solutions of former solved subproblems to find whether $B$ and $C$ yield them.
If for any splitting point $k$ of the current substring, $0 < k < j$, $B$ yields the left part of the split substring, $B\Rightarrow s[i..i+k]$, and $C$ yields the left part, $B\Rightarrow s[i+k+1..i+j]$, then $A\xRightarrow{*} s[i..i+j]$.

For each $tab[i,j]$ with $2\leq j \leq n$ and $1\leq i \le n-j$ the algorithm iterates over all non-terminals $A\in V$ and their productions $A\rightarrow BC \in R$.
For each rule, it iterates over all possible splitting points $k$, $1\leq k\le j$.
If $B$ is in $tab[i, k]$ and $C$ in $tab[i+k,j-k]$, then $A$ is added to $tab[i,j]$.

The technique of dividing the problem into smaller subproblems and use their solutions to solve the problem is called Dynamic programming.

\subsubsection{Dynamic Programming}
\todo{introduce dynammic programming, say how it works.}

CYK applies the dynamic programming technique by dividing the problem into two smaller subproblems and solving them each respectively.

In order for dynamic programming to be applicable on a problem, the problem must fulfil two requirements; optimal substructure and overlapping subproblems.
Optimal substructure says, that the optimal solution of a problem can be build from the optimal solutions of a set of subproblem. 
The membership problem looks for a truth-value, the optimal value is therefore true.
When trying to find the truth-value for a non-terminal variable $A$ for any string, we thus try to find any combination of a splitting point $k$ and a production in $R$ with $A$ on its left-hand side, for which both subproblems (the left and right substring), are optimal, i.e., true.
If we find such a combination, we can use the answers of both subproblem; the answer of our problem is the logical \textit{and} of the solution of both subproblems.

To proof that the problem has overlapping subproblems, we will give a small example.
Assume we run the algorithm on an input string of length 6, $s[1..6]$, and our grammar contains a non-terminal variable $A$ with $(A\rightarrow BC), (A\rightarrow CB)\in R$.
At some point, the algorithm might check if $A$ can yield certain substrings of length 4, i.e., substrings $s[1..4]$ and $s[2..6]$.
When considering the first rule, $(A\rightarrow BC)\in R$, on $s[1..4]$, it will check whether $B$ yields $s[1..2]$ and $C$ yields $s[3..4]$.
When considering the second rule, $(A\rightarrow CB)\in R$, on $s[2..6]$, it will check whether $C$ yields $s[3..4]$ and $C$ yields $s[5..6]$.
Finding the truth-value for $C$ on $s[3..4]$ is a subproblem, which will be solved twice, and therefore overlapping.
\todo{draw a small tree to visualize this}

The CYK algorithm has a very good worst-case running time of $O(n^3 * |G|)$.
In practice there are algorithms with a better average case running times.
In the following subsections, we go into more detail on the running time, while introducing three different parsing algorithms, which were used for the evaluation.
The first one is a naive approach, the second one the original CYK-algorithm, and the third one a top-down approach, which makes the naive approach more efficient by introducing memoization.



\subsubsection{Naive}

The naive approach is a recursive depth-first implementation.
It does not use dynamic programming, therefore each subproblem may get solved multiple times.
The input string will be stored globally, as an array of characters $s[1..n]$.
The procedure takes three input arguments, a non-terminal $A$ and the starting and end points of the substring, $i$ and $j$, which should be considered.
If it is called on a substring of length 1, i.e., $i = j-1$, it returns the truth-value of whether $A\rightarrow s[i]$ holds.
This is done in lines 1 to 7 in~\ref{alg:naive}.
Otherwise, it iterates over all non-terminal rules of $A$, $(A\rightarrow BC) \in R$, trying to find a splitting point $k$, $i <= k < j$, for which $B$ yields $s[i..k]$ and $C$ yields $s[k+1..j]$.
If no such rule can be found, the algorithm returns false, since $A$ can not yield $s[i..j]$.
The initial call on the method is \texttt{Naive($S$, $0$, $n$)}.

\begin{algorithm}[H]
    \caption{Naive Parser}
    \label{alg:naive}
    \begin{algorithmic}[1]
        \Procedure{Naive}{non-terminal A, int i, int j}
        \If{$i = j-1$}
            \If{$(A\rightarrow s[i])\in R$}
                \State \textbf{return} true
            \Else
                \State \textbf{return} false
            \EndIf
        \EndIf

        \For{$(A\rightarrow BC) \in R$}
            \For{$k \in \{i+1,\dots,j-1\}$}
                \If {\Call{Naive}{$B$, $i$, $k$} \textbf{and} \Call{Naive}{$C$, $k+1$, $j$}}
                    \State \textbf{return} true
                \EndIf
            \EndFor
        \EndFor

        \State \textbf{return} false
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The complexity of this algorithm is exponential in $n$, the length of the input string\todo{add reference to book}.
We expect this approach to be the slowest of the three.


\subsubsection{Bottom-Up}
This is the original CYK-algorithm.
It initializes an empty table $tab$ of size $|V|\times n\times n$.
When the algorithm is finished, $tab[A,i,j]$ will be true, if non-terminal $A$ can yield $s[i..i+j]$.
Notice that $j$ is no longer the end point of the substring, but its length.

Since the algorithm performs bottom-up, it first fills the bottom row of the table, i.e. $tab[A,i,1]$ for $i\in{1,\dots,n}$ and all $A\in V$.
It then iteratively increases $j$, and fills all cells on its way up through the table.
At each cell $tab[A,i,j]$, the algorithm checks if there is a rule $A\rightarrow BC$ in $R$ and a $k$, $1 \leq k < j$, for which $tab[B,i,k]$ and $tab[C,i+k+1, j]$ are both true.
This way, it uses the solutions to already solved subproblems to solve the current problem, only accessing cells of $tab$, which were already filled before.
If so, $A$ can yield $s[i..i+j]$, and $tab[A,i,j]$ will is set to true.
Since the algorithms were implemented in Java, $tab[C,i+k+1, j]$ will only be accessed if $tab[B,i,k]$ is true.


\begin{algorithm}[H]
    \caption{Bottom-Up CYK Parser}
    \label{alg:bu}
    \begin{algorithmic}[1]
        \Procedure{Bottom-Up}{input string $s[1..n]$}
        \State allocate table $tab[|V|][n][n]$ initialized with false
    
        \For{$i \in {1,\dots,n}$}
            \For{${A: A\rightarrow s[i]\in R}$}
                \State $tab[A,i,1] \leftarrow$ true
            \EndFor
        \EndFor

        \For{$j\in {2,\dots,n}$} \hspace*{2.75cm}\textit{-- length of substring}
            \For{$i\in {1,\dots,n-j+1}$} \hspace*{1cm}\textit{-- starting point of substring}
                \For{$(A\rightarrow BC) \in R$} \hspace*{1cm}\textit{-- all productions}
                    \For{$k \in {1,\dots,j-1}$} \hspace*{0.5cm}\textit{-- all splitting points}
                        \If {$tab[B,i,k]$ \textbf{and} $tab[C,i+k,j-k]$}
                            \State $tab[A,i,j]\leftarrow$ true
                            \State break loop
                        \EndIf
                    \EndFor
                \EndFor
            \EndFor
        \EndFor

        \State \textbf{return} $tab[S,1,n]$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The bottom-up CYK algorithm solves each subproblem exactly once.
It has a complexity of $O(n^3)$\todo{reference book}.
The initialization, lines 3 to 5, takes $O(n)$, due to the iteration over all elements of $s$.
The for-loop on line 9 is repeated at most $n$ times since $k$ is in the interval $\{1,\dots,n\}$ at most (in practice, it will often be executed less, since it breaks, as soon as the condition is met).
The two outer loops, lines 6 and 7, are both repeated $n$ times.
Thus, the loop at line 9 will be called $O(n^2)$ times, which results in a complexity of $O(n^3)$ for lines 6 to 12.
The overall running time is therefore in $O(n^3)$.

We expect this algorithm to behave very similarly on strings of the same length.
Further, the order in which the rules of the grammar are provided does have an impact, but should not affect the running time as much as it does for the top down approach, which we show next.

\subsubsection{Top-Down}
\label{sec:top_down}
The top-down approach resembles the naive one, as it is recursive.
It uses, however, memoization, which makes it a lot more efficient, as each subproblem is solved once at most.
When the method \texttt{Top-Down-Parse(input string $s[1..n]$)}(Algorithm~\ref{alg:td1}) is called, it initializes the global table of size $|v|\times n\times n$, which is similar to the one used for the bottom-up CYK algorithm.
It then calls \texttt{Top-Down($S$, $1$, $n$)}(Algorithm~\ref{alg:td}) and returns $tab[S,1,n]$, which contains the truth value of the membership problem.
\texttt{Top-Down($A$, $i$, $j$)} first checks, whether the subproblem of whether $A$ yields $s[i..j]$ was already solved, i.e., if $tab[A,i,j]$ is set.
If so, it returns the before computed truth-value.
Otherwise, the value is computed recursively, stored in $tab[A,i,j]$ and returned.
The next call of \texttt{Top-Down($A$, $i$, $j$)} will not compute anything, but return the truth-value immediately.

Similar to bottom-up, the right-hand side of the if-request on line 11 will only be called, if the left-hand side is true.

\begin{algorithm}[H]
    \caption{Top-Down Parser}
    \label{alg:td1}
    \begin{algorithmic}[1]
        \Procedure{Top-Down-Parse}{input string $s[1..n]$}
        \State allocate global table $tab[|V|][n][n]$ initialized with null
        \State \Call{Top-Down-Parser}{$S$, $1$, $n$}
        \State \textbf{return} $tab[S,1,n]$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
    \caption{Top-Down}
    \label{alg:td}
    \begin{algorithmic}[1]
        \Procedure{Top-Down}{non-terminal A, int i, int j}
            \If{$tab[A,i.j]=$\texttt{null}}
                \State \textbf{return} $tab[A,i,j]$
            \EndIf

            \State $tab[A,i.j]\leftarrow$ false

            \If{$j = 0$}
                \If{$(A\rightarrow s[i])\in R$}
                    \State $tab[A,i.j]\leftarrow$ true
                \EndIf
            \Else
                \For{$(A\rightarrow BC) \in R$}
                    \For{$k \in \{i+1,\dots,j-1\}$}
                        \If {\Call{Top-Down}{$B$, $i$, $k$} \textbf{and} \Call{Top-Down}{$C$, $i+k$, $j-k$}}
                        \State $tab[A,i.j]\leftarrow$ true
                        \State break loop
                        \EndIf
                    \EndFor
                \EndFor
            \EndIf

            \State \textbf{return} $tab[A,i.j]$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

The complexity of this algorithm is, similar to the bottom-up algorithm, $O(n^3)$.
The difference between the two is, that bottom-up fills all cells of $tab$, while top-down only fills the ones it passes while trying to find a solution.
In practice, its running time is therefore more dependant on the input string itself, as well as the grammar.
Depending of the order of the rules, it may yield very different running times.
If it consults rules, that yield the considered substrings in the beginning, it does not consult other rules, and must therefore solve a lot less subproblems.
If this is not the case, and most of the subproblems must be solved, then we expect the algorithm to be slower than bottom-up.

\subsubsection{Implementation}
Write a little bit about what data structures where used to represent the productions etc. Necessary?


% \todo{The parser was implemented in Java.
% It can be called with instructions on the set on input string, also called test set, and the algorithm which should be used.

% The rules of the grammar were implemented in a way, that the algorithms can easily access all rules of a variable and iterate over them.
% The non-terminal rules of each variable in $V$ are stored in an array of arrays of integers.
% The arrays of each variable are referenced from an array \texttt{rules}.
% If the start variable has one rule, say $v_0 \rightarrow v_1v_2$, then \texttt{rules[0][0] = [1, 2]}

% The terminal rules are stored twice, once as rules of the terminal symbols and once as the rules of the non-terminal variables, making both [reference algorithms] efficient.
% }


