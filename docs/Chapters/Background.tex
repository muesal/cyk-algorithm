\section{Background}
\label{sec:backgrund}

The Cocke-Younger-Kasami algorithm, which we analyse in this report, solves the membership problem for context free grammars.
In this section we show what a context-free grammar is and how the algorithm operates on it.
Further, we introduce the three approaches for implementing the algorithm which were used in the evaluation.

\subsection{Context-Free Grammar}
\textbf{Context-free grammars} (CFG) are used to formalize different types of languages.
They can, for example, be used in computer science, to define the structure of programming languages or in linguistics to define the structure of any language.

A CFG contains a set of rules, also called productions.
Starting from a certain variable, this productions can be applied to get a sequence of terminal symbols, for example a sentence of the English language.
The sequences that can be generated with a CFG build a language, the grammars \textbf{context-free language} (CFL).

Formally, we define a CFG $G$ by the 4-tuple $G=(V,T,P,S)$.
$V$ and $T$ are two finite, disjoint sets containing all \textbf{variables} and \textbf{terminal} symbols respectively.
The variables are also called non-terminals.
$P$ is the set of \textbf{productions} and $S\in V$ is the \textbf{start symbol}.

The productions are of the form $A\rightarrow\alpha$, where $A$ is a variable in $V$ and $\alpha$ is a string of symbols from $(V\cup T)^*$.
If $P$ contains multiple productions for one non-terminal we abbreviate these productions as $A\rightarrow \alpha_1 | \alpha_2 | \dots |\alpha_k $, where $\alpha_i$, $i\in {1\dots k}$ is the right hand side of one of the productions for non-terminal $A$.

If for any strings $u,v\in (V\cup T)^*$ there is a production which can be applied to $u$ such that the result is $v$ we say $u$ \textbf{directly derives} $v$, denoted as $u\Rightarrow v$.
For example, applying rule $B->bc$ on the string $aB$ directly derives the terminal string $abc$, i.e. $aB \Rightarrow abc$.
If $v$ can be reached by applying multiple productions on $u$ we say $u$ \textbf{derives} $v$, denoted as \smash{$u\xRightarrow{*} v$}, i.e., there is a set of strings $u_1, u_2, \dots, u_k\in (V\cup T)^*)$ such that $u\Rightarrow u_1 \Rightarrow u_2 \Rightarrow \dots \Rightarrow u_k \Rightarrow v$.
If we enhance our example by the rule $S\rightarrow aB$, then \smash{$S\xRightarrow{*}abc$}, by applying the rule of $S$ first, and then the rule of $B$.

The language generated by $G$, $L(G)$, contains all strings that can be yielded from $S$, i.e. $L(G)={w\in T^*: S\xRightarrow{*}w}$.
The \textbf{membership problem}, which is solved by the CYK-algorithm, is the problem of determining whether a given string is in the language of a grammar.

The following subsections show one simple example of a context-free grammar and introduce a special of grammars.

\subsubsection{Exemplary Grammar}
\label{subsec:exemplary_grammar}
One simple example for a context free grammar is one, whose language consists of all words of the form $(a^n b^n)$, for any $n\in \mathbb{N}$.
This grammar can be defined as $G = (\{S\}, \{a, b\}, P, S)$, where $P$ contains the following production: 

\begin{align*}
S&\rightarrow aSb | ab \\
\end{align*}

Any string of length $2n$ can be generated by applying $S\rightarrow aSb$ $n-1$ times and $S\rightarrow ab$ once in the end.


\subsubsection{Chomsky Normal Form}

Every context-free grammar can be transformed into an \textbf{equivalent} representation in \textbf{Chomsky normal form} (CNF).
Two grammars are considered equivalent if they generate the same language.
A grammar is in CNF, if all its productions are of the form:

\hspace*{0.5cm}$A\rightarrow BC$\\
\hspace*{1cm}$A\rightarrow a$\\
\hspace*{1cm}$S\rightarrow \epsilon$\\

While $S$ is the start symbol, $A$, $B$ and $C$ are any variables in $V$, but neither $B$ nor $C$ may be the start symbol.
$a$ is a terminal variable.
The start symbol is the only variable, which may derive the empty string, provided the empty string is part of the language.
Further, a non-terminal must either derive two non terminals or one terminal variable.

The CYK algorithm can only operate on grammars, that are in the \textbf{reduced Chomsky normal form}.
The reduced CNF is similar to CNF, with the only difference that $S$ may also appear on the right hand side of a production.

In order to transform the grammar from section~\ref{subsec:exemplary_grammar} to reduced CNF, we add the non-terminals $C,D$ to $V$ and get the new set of productions $P'$:
\begin{align*}
    S&\rightarrow AC | AB \\
    C&\rightarrow SB \\
    A&\rightarrow a \\
    B&\rightarrow b \\
\end{align*}

In~\ref{sec:dynamic_programming} we show, why CYK depends on grammars to be in reduced CNF.

\subsection{Cocke-Younger-Kasami Algorithm}

The \textbf{Cocke-Younger-Kasami} algorithm (CYK) solves the membership problem~\cite{automata}.
For a given Grammar $G$ and an input string $s[1..n]$, it returns the truth-value of whether $s$ is in $L(G)$.
For the original algorithm, $G$ must be in reduced CNF or CNF.

The algorithm solves the membership problem in a bottom up manner.
It maintains a table $tab$ of size $n\times n$, where $tab[i,j]$ contains all non-terminals that can derive the substring of $s$ of length $j$ starting at position $i$.

First, $tab$ is initialized as an empty $n\times n$-matrix.
The algorithm then starts by looking over all substrings of size 1, to find the non-terminals that produce the terminals of the input string.
Since the grammar is in CNF, at each terminal production one non-terminal derives exactly one terminal, therefore this can be done in a straight forward manner.
When this is done, $tab[i,1]=\{A: A\in V \text{ and } A\rightarrow s[i] \in P\}$ for all $i\in {1\dots n}$.
The algorithm continues by increasing $j$, from $2$ to $n$, and iterating over all possible $i$, i.e., $1\leq i < n-j$.
Since the algorithm proceeds bottom-up, when looking at a substring of length $j$, the solution for all strings of length $j-1$ is already known.
Thus, to deduce whether non-terminal $A\in V$ can derive a substring $s[i...i+j]$ the algorithm iterates over all non-terminal productions of $A$.
For each production $A\rightarrow BC$ of $A$ in $P$ it uses the solutions of former solved subproblems to find whether $A\rightarrow BC$ derives the substring.
If for any splitting point $k$ of the current substring, $0 < k < j$, $B$ derives the left part of the split substring, $B\Rightarrow s[i..i+k]$, and $C$ derives the left part, $B\Rightarrow s[i+k+1..i+j]$, then \smash{$A\xRightarrow{*} s[i..i+j]$}.

For each $tab[i,j]$ with $2\leq j \leq n$ and $1\leq i \le n-j$ the algorithm iterates over all non-terminals $A\in V$ and their productions $A\rightarrow BC \in P$.
For each production, it iterates over all possible splitting points $k$, $1\leq k\le j$.
If $B$ is in $tab[i, k]$ and $C$ in $tab[i+k,j-k]$, then $A$ is added to $tab[i,j]$.

The technique of dividing the problem into smaller subproblems and use their solutions to solve the problem is called Dynamic programming.

\subsubsection{Dynamic Programming}
\label{sec:dynamic_programming}
\textbf{Dynamic programming} is a technique to solve optimization problems~\cite{cormen2009introduction}.
The problems are solved, by combining the solutions to subproblems.
It is typically applied on problems, where certain subproblems must be solved multiple times in order to find the solution to a problem.
To solve them only once, a table is created that stores the solution to subproblems.
This method thus performs a memory-time tradeoff; it uses memory to save computation time.

There are two different approaches on how to apply dynamic programming.
\textbf{Bottom-up} orders subproblems by size and then solves the smallest first.
It then uses the solutions to these subproblems to solve the bigger ones, thus it fills the table from bottom to top, solving all subproblems the problem has.
Whenever a new subproblem is solved, all of its subproblems have already been solved.

\textbf{Top-down with memoization} on the other hand solves the problem recursively.
Thus, if the method is called on a subproblem small enough it solves it directly.
Otherwise it divides it into more subproblems and calls itself on them.
However, at each recursive call it first accesses the table to see, if the current subproblem has been solved before.
If so, this solution is returned.
If not, it continues as usual, but the found solution is stored in the table.
When this subproblem appears the next time, it must not be computed again.

In order for dynamic programming to be applicable on a problem, the problem must fulfil two requirements.
The first, \textbf{optimal substructure}, says, that the optimal solution of a problem can be build from the optimal solutions of a set of subproblem.
\textbf{Overlapping subproblems} is the second requirement, assuring that the problem has overlapping subproblems.

CYK applies the bottom-up approach, by dividing the string into two smaller substrings and solving them each respectively.
The membership problem looks for a truth-value, for a subproblem are therefore only two possible answers, true or false.
The optimal value is true, as the algorithm tries to find a combination of subproblems, that returns true.
When trying to find the truth-value for a non-terminal variable $A$ for any string, we try to find any combination of a splitting point $k$ and a non-terminal production in $P$ for $A$, $A\rightarrow BC$, that derives the string.
The resulting subproblems are whether $B$ and $C$ can derive the left and right substring respectively.
We can do this for all non-terminal productions, as they all have exactly two non-terminal variables on their right-hand side, since the grammar is in reduced CNF.
The answer of our problem is the logical \textit{and} of the solution of both subproblems, i.e., if for any such combination both subproblems return true, then $A$ can derive the string.
This already shows, that the optimal substructure holds for the membership problem.

\input{Resources/recursive_tree.tex}

To show that the problem has overlapping subproblems, we will use the tree in figure \ref{fig:recursive-tree}, which shows all subproblems of a string of length four.
The total amount of subproblems is 26.
Therefore, any algorithm trying to solve every subproblem computes 26 subproblems.
The nodes of the tree are colored, such that the same subproblems have the same color, i.e., the problem has only nine distinct subproblems.
An algorithm using memoization thus solves nine subproblems, and reuses the solution for the remaining 17.
This tree is a simplification, as for the algorithms presented in the following subsections the actual trees would have more layers, since they compute the subproblems with regard to given non-terminal variables.
Even though the numbers differ, it should become clear that there may be a big number of non-distinct subproblems.
This shows, that the membership problem indeed has overlapping subproblems.

The CYK algorithm has a very good worst-case running time of $O(n^3 * |G|)$.
In practice there are algorithms with a better average case running times.
In the following subsections, we go into more detail on the running time, while introducing three different parsing algorithms, which were used for the evaluation.
The first one is a na\"{i}ve approach, the second one the original CYK-algorithm, and the third one a top-down approach, which makes the na\"{i}ve approach more efficient by introducing memoization.

\subsubsection{Na\"{i}ve}

The na\"{i}ve approach is a recursive depth-first implementation.
It does not use dynamic programming, therefore each subproblem may get solved multiple times.
The input string will be stored globally, as an array of characters $s[1..n]$.
\texttt{counter} is a global variable which is initialized with $0$ and increased at every recursive call.
The procedure takes three input arguments: a non-terminal $A\in V$ and the starting and end points ($i$ and $j$) of the substring, which should be considered.
If it is called on a substring of length 1, i.e., $i = j-1$, it returns the truth-value of whether $A\rightarrow s[i]$ holds.
This is done in lines~\ref{lst:naive_1} t~\ref{lst:naive_7} in Algorithm~\ref{alg:naive}.
Otherwise, it iterates over all non-terminal productions of $A$, $(A\rightarrow BC) \in P$.
For each production it tries to find a splitting point $k$, $i <= k < j$, for which $B$ yields $s[i..k]$ and $C$ yields $s[k+1..j]$, using recursive calls.
If no such production can be found, the algorithm returns false, since $A$ can not derive $s[i..j]$.
The initial call on the method is \texttt{Naive($S$, $1$, $n$)}.

\begin{algorithm}[H]
    \caption{Naive Parser}
    \label{alg:naive}
    \begin{algorithmic}[1]
        \Function{Naive}{non-terminal A, int i, int j}
        \State counter $\leftarrow$ counter + 1
        \If{$i = j$} \label{lst:naive_1}
            \If{$(A\rightarrow s[i])\in P$}
                \State \textbf{return} true \label{lst:naive_ret_1}
            \Else
                \State \textbf{return} false \label{lst:naive_ret_2}
            \EndIf
        \EndIf \label{lst:naive_7}

        \For{$(A\rightarrow BC) \in P$}
            \For{$k \in \{i,\dots,j-1\}$}
                \If {\Call{Naive}{$B$, $i$, $k$} \textbf{and} \Call{Naive}{$C$, $k+1$, $j$}}
                    \State \textbf{return} true
                \EndIf
            \EndFor
        \EndFor

        \State \textbf{return} false
        \EndFunction
    \end{algorithmic}
\end{algorithm}


Imagine, that instead of all possible splitting points $k$ the algorithm would only try the first and the last one.
This would result in four recursive calls.
Two of those are on the substrings of length 1, and run in linear time, as the immediately return a result (\ref{lst:naive_ret_1} or~\ref{lst:naive_ret_2})
The other two recursive calls would again each call 4 recursive calls, of which two are linear.
This is repeated, until a substring of size 2 is reached, for which all calls return in linear time.
This would result in a total of $2^n$ recursive calls, thus the running time of such an algorithm would be in $O(2^{n-1})$.
However, Algorithm~\ref{alg:naive} does not try 2 but $n-1$ splitting points in worst case, and thus performs $2*(n-1)$ recursive calls.
The amount of subproblems tht are considered this way is $(n-1)/2 * O(2^{n-1})$.
Thus, the running time is exponential in $n$~\cite{automata}.

The following algorithms show, how the running time can be improved by using memoization.


\subsubsection{Bottom-Up}
\label{sec:bottom_up}
The bottom-up approach is the original CYK-algorithm.
It initializes a table $tab$ of size $|V|\times n\times n$.
When the algorithm is finished, $tab[A,i,j]$ will be true, if non-terminal $A$ can derive $s[i..i+j]$.
Notice that $j$ is no longer the end point of the substring, but its length.
Further, it initializes a \texttt{counter}, which holds the number of iterations on the innermost loop.

Since the algorithm performs bottom-up, it first fills the bottom row of the table, i.e. $tab[A,i,1]$ for $i\in\{1,\dots,n\}$ and all $A\in V$.
It then iteratively increases $j$, and fills all cells on its way up through the table.
At each cell $tab[A,i,j]$, the algorithm checks if there is a production $A\rightarrow BC$ in $P$ and a $k$, $1 \leq k < j$, for which $tab[B,i,k]$ and $tab[C,i+k+1, j]$ are both true.
This way, it uses the solutions to already solved subproblems to solve the current problem, only accessing cells of $tab$, which were already filled before.
If so, $A$ can derive $s[i..i+j]$, and $tab[A,i,j]$ will is set to true.
Since the algorithms were implemented in Java, $tab[C,i+k+1, j]$ will only be accessed if $tab[B,i,k]$ is true. \todo{move to implementation if existing}


\begin{algorithm}[H]
    \caption{Bottom-Up CYK Parser}
    \label{alg:bu}
    \begin{algorithmic}[1]
        \Function{Bottom-Up}{input string $s[1..n]$}
        \State allocate table $tab[|V|][n][n]$ initialized with false
        \State counter $\leftarrow$ 0 \label{lst:bu_ini_1}
    
        \For{$i \in {1,\dots,n}$}
            \For{${A: A\rightarrow s[i]\in P}$}
                \State $tab[A,i,1] \leftarrow$ true
            \EndFor
        \EndFor \label{lst:bu_ini_2}

        \For{$j\in \{2,\dots,n\}$} \hspace*{2.75cm}\textit{-- length of substring} \label{lst:bu_for_1}
            \For{$i\in \{1,\dots,n-j+1\}$} \hspace*{1cm}\textit{-- starting point of substring} \label{lst:bu_for_2}
                \For{$(A\rightarrow BC) \in P$} \hspace*{1cm}\textit{-- all productions} \label{lst:bu_for_3}
                    \For{$k \in \{1,\dots,j-1\}$} \hspace*{0.5cm}\textit{-- all splitting points} \label{lst:bu_for_4}
                        \State counter $\leftarrow$ counter + 1
                        \If {$tab[B,i,k]$ \textbf{and} $tab[C,i+k,j-k]$}
                            \State $tab[A,i,j]\leftarrow$ true
                            \State break loop
                        \EndIf
                    \EndFor
                \EndFor
            \EndFor
        \EndFor

        \State \textbf{return} $tab[S,1,n]$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The bottom-up CYK algorithm solves each subproblem exactly once.
It has a complexity of $O(n^3)$~\cite{automata}.
The initialization, lines~\ref{lst:bu_ini_1} to~\ref{lst:bu_ini_2}, takes $O(n)$, due to the iteration over all elements of $s$.
The for-loop on line~\ref{lst:bu_for_4} is repeated at most $n$ times since $k$ is in the interval $\{1,\dots,n\}$ at most (in practice, it will often be executed less, since it breaks, as soon as the condition is met).
The two outer loops, lines~\ref{lst:bu_for_1} and~\ref{lst:bu_for_2}, are both repeated $n$ times.
Thus, the loop at line~\ref{lst:bu_for_4} will be called $O(n^2)$ times, which results in a complexity of $O(n^3)$ for lines 6 to 12.
The overall running time is therefore in $O(n^3)$.

We expect this algorithm to behave very similarly on strings of the same length.
Further, the order in which the productions of the grammar are provided does have an impact, but should not affect the running time as much as it does for the top down approach, which we show next.

\subsubsection{Top-Down}
\label{sec:top_down}
The top-down approach resembles the na\"{i}ve one, as it is recursive.
It uses, however, memoization, which makes it a lot more efficient as each subproblem is solved once at most.
When the method \texttt{Top-Down-Parse($s[1..n]$)}) is called, it initializes the global table of size $|v|\times n\times n$, which is similar to the one used for the bottom-up CYK algorithm.
It then calls \texttt{Top-Down($S$, $1$, $n$)} and returns $tab[S,1,n]$, which contains the truth value of the membership problem.
\texttt{Top-Down($A$, $i$, $j$)} first checks, if the subproblem of whether $A$ derives $s[i..j]$ was already solved, i.e., if $tab[A,i,j]$ is set.
If so, it returns the before computed truth-value.
Otherwise, the value is computed recursively, stored in $tab[A,i,j]$ and returned.
The next call of \texttt{Top-Down($A$, $i$, $j$)} will not compute anything, but return the truth-value immediately.
Like the na\"{i}ve approach, the top-down algorithm has a \texttt{counter} which holds the number of calls on the recursive function.

Similar to bottom-up, the right-hand side of the if-request on line 11 will only be called, if the left-hand side is true. \todo{at implementation, if written}

\begin{algorithm}[H]
    \caption{Top-Down Parser}
    \label{alg:td}
    \begin{algorithmic}[1]
        \Function{Top-Down-Parse}{input string $s[1..n]$}
        \State allocate global table $tab[|V|][n][n]$ initialized with null
        \State counter $\leftarrow$ 0
        \State \Call{Top-Down-Parser}{$S$, $1$, $n$}
        \State \textbf{return} $tab[S,1,n]$
        \EndFunction
        \Function{Top-Down}{non-terminal A, int i, int j}
            \State counter $\leftarrow$ counter + 1

            \If{$tab[A,i.j]=$\texttt{null}}
                \State \textbf{return} $tab[A,i,j]$
            \EndIf

            \State $tab[A,i.j]\leftarrow$ false

            \If{$j = 0$}
                \If{$(A\rightarrow s[i])\in P$}
                    \State $tab[A,i.j]\leftarrow$ true
                \EndIf
            \Else
                \For{$(A\rightarrow BC) \in P$}
                    \For{$k \in \{i+1,\dots,j-1\}$}
                        \If {\Call{Top-Down}{$B$, $i$, $k$} \textbf{and} \Call{Top-Down}{$C$, $i+k$, $j-k$}} \label{lst:td_call}
                        \State $tab[A,i.j]\leftarrow$ true
                        \State break loop
                        \EndIf
                    \EndFor
                \EndFor
            \EndIf

            \State \textbf{return} $tab[A,i.j]$
        \EndFunction
    \end{algorithmic}
\end{algorithm}

The complexity of this algorithm is, similar to the bottom-up algorithm, $O(n^3)$.
The difference between the two is, that bottom-up fills all cells of $tab$, while top-down only fills the ones it passes while trying to find a solution.
In practice, its running time is therefore more dependant on the input string itself, as well as the grammar.
Depending of the order of the productions, it may derive very different running times.
If it consults productions that derive the considered substrings in the beginning, it does not consult other productions, and must therefore solve a lot less subproblems.
If this is not the case, and most of the subproblems must be solved, then we expect the algorithm to be slower than bottom-up.

In the following sections we first show a specializations and a generalization of the CYK algorithm, before we evaluate them and the conventional implementation.
 
%\subsubsection{Implementation}

% \todo{The parser was implemented in Java.
% It can be called with instructions on the set on input string, also called test set, and the algorithm which should be used.

% The productions of the grammar were implemented in a way, that the algorithms can easily access all productions of a variable and iterate over them.
% The non-terminal productions of each variable in $V$ are stored in an array of arrays of integers.
% The arrays of each variable are referenced from an array \texttt{productions}.
% If the start variable has one production, say $v_0 \rightarrow v_1v_2$, then \texttt{productions[0][0] = [1, 2]}

% The terminal productions are stored twice, once as productions of the terminal symbols and once as the productions of the non-terminal variables, making both [reference algorithms] efficient.
% }


